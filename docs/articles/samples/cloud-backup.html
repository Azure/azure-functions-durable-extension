<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Fan-In/Fan-Out - Cloud Backup </title>
    <!-- Prevent search engine web crawlers from indexing or crawling this page -->
    <meta name="robots" content="none">
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="Fan-In/Fan-Out - Cloud Backup ">
    <meta name="generator" content="docfx 2.28.1.0">
    
    <link rel="shortcut icon" href="../../images/favicon.ico">
    <link rel="stylesheet" href="../../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../../styles/docfx.css">
    <link rel="stylesheet" href="../../styles/main.css">
    <meta property="docfx:navrel" content="../../toc.html">
    <meta property="docfx:tocrel" content="../toc.html">
    
    
  </head>
  <body data-spy="scroll" data-target="#affix">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../../index.html">
                <img id="logo" class="svg" src="../../logo.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <form class="navbar-form navbar-right" role="search" id="search">
                <div class="form-group">
                  <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off">
                </div>
              </form>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="">

<div class="WARNING"><h5>Warning</h5><p>This documentation is out of date and will be deleted. The official documentation has been moved <a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-cloud-backup">here</a>.</p>
</div>
<h1 id="fan-infan-out---cloud-backup">Fan-In/Fan-Out - Cloud Backup</h1>
<p>Fan-in/fan-out is a more advanced use-case which demonstrates how to write an orchestrator function that calls multiple functions concurrently and then performs some aggregation on the results. In particular, this sample demonstrates how the pattern can be used to create a durable function that backs up all or some of an app&#39;s site content into Azure Storage.</p>
<h2 id="before-you-begin">Before you begin</h2>
<p>If you haven&#39;t done so already, make sure to read the <a href="../overview.html">overview</a> before jumping into samples. It will really help ensure everything you read below makes sense.</p>
<p>All samples are combined into a single function app package. To get started with the samples, follow the setup steps below that are relevant for your development environment:</p>
<h3 id="for-visual-studio-development-windows-only">For Visual Studio Development (Windows Only)</h3>
<ol>
<li>Download the <a href="../../files/VSDFSampleApp.zip">VSDFSampleApp.zip</a> package, unzip the contents, and open in Visual Studio 2017 (version 15.3).</li>
<li>Install and run the <a href="https://docs.microsoft.com/en-us/azure/storage/storage-use-emulator">Azure Storage Emulator</a>. Alternatively, you can update the <code>local.appsettings.json</code> file with real Azure Storage connection strings.</li>
<li>The sample can now be run locally via F5. It can also be published directly to Azure and run in the cloud.</li>
</ol>
<h3 id="for-azure-portal-development">For Azure Portal Development</h3>
<ol>
<li>Create a new function app at <a href="https://functions.azure.com/signin">https://functions.azure.com/signin</a>.</li>
<li>Follow the <a href="../installation.html">installation instructions</a> to configure Durable Functions for portal development.</li>
<li>Download the <a href="../../files/DFSampleApp.zip">DFSampleApp.zip</a> package.</li>
<li>Unzip the sample package file into <code>D:\home\site\wwwroot</code> using Kudu or FTP.</li>
</ol>
<p>The code snippets and binding references below are specific to the portal experience, but have a direct mapping to the equivalent Visual Studio development experience.</p>
<p>This article will specifically walk through the following functions in the sample app:</p>
<ul>
<li><strong>E2_BackupSiteContent</strong></li>
<li><strong>E2_GetFileList</strong></li>
<li><strong>E2_CopyFileToBlob</strong></li>
</ul>
<div class="NOTE"><h5>Note</h5><p>This walkthrough assumes you have already gone through the <a href="sequence.html">Hello Sequence</a> sample walkthrough. If you haven&#39;t done so already, it is recommended to first go through that walkthrough before starting this one.</p>
</div>
<h2 id="scenario-overview">Scenario overview</h2>
<p>In this sample, we write a function which uploads all files under a specified directory recursively into blob storage. We also count the total number of bytes that were uploaded to blob storage as part of the process.</p>
<p>The naive way to implement this solution is to write a single function to take care of everything. The main problem you&#39;ll run into is <strong>scalability</strong>. A single function execution can only run on a single VM, so the throughput will be limited by the throughput of that single VM. Another problem is <strong>reliability</strong>. If there is a failure midway through, or if the entire process takes more than 5 minutes, then the backup will fail in a partial state and needs to be restarted.</p>
<p>A more robust approach would be to write two regular functions: one which enumerates the files and adds the file names to a queue, and another function which reads from the queue and uploads the files to blob storage. This is better in terms of throughput and reliability, but requires you to provision and manage a queue. More importantly, significant complexity is introduced in terms of <strong>state management</strong> and <strong>coordination</strong> if you want to do anything more, like report the total number of bytes uploaded.</p>
<p>A Durable Functions approach gives you all of the mentioned benefits with very low overhead.</p>
<h2 id="the-cloud-backup-orchestration">The cloud backup orchestration</h2>
<p>The <strong>E2_BackupSiteContent</strong> function uses the standard function.json for orchestrator functions.</p>
<pre><code class="lang-json" name="Main">{
  &quot;bindings&quot;: [
    {
      &quot;name&quot;: &quot;backupContext&quot;,
      &quot;type&quot;: &quot;orchestrationTrigger&quot;,
      &quot;direction&quot;: &quot;in&quot;
    }
  ],
  &quot;disabled&quot;: false
}
</code></pre><p>Here is the code which implements the orchestrator function:</p>
<pre><code class="lang-csharp" name="Main">#r &quot;Microsoft.Azure.WebJobs.Extensions.DurableTask&quot;

public static async Task&lt;long&gt; Run(DurableOrchestrationContext backupContext)
{
    string rootDirectory = Environment.ExpandEnvironmentVariables(backupContext.GetInput&lt;string&gt;() ?? &quot;&quot;);
    if (string.IsNullOrEmpty(rootDirectory))
    {
        throw new ArgumentException(&quot;A directory path is required as an input.&quot;);
    }

    if (!Directory.Exists(rootDirectory))
    {
        throw new DirectoryNotFoundException($&quot;Could not find a directory named &#39;{rootDirectory}&#39;.&quot;);
    }

    string[] files = await backupContext.CallActivityAsync&lt;string[]&gt;(
        &quot;E2_GetFileList&quot;,
        rootDirectory);

    var tasks = new Task&lt;long&gt;[files.Length];
    for (int i = 0; i &lt; files.Length; i++)
    {
        tasks[i] = backupContext.CallActivityAsync&lt;long&gt;(
            &quot;E2_CopyFileToBlob&quot;,
            files[i]);
    }

    await Task.WhenAll(tasks);

    long totalBytes = tasks.Sum(t =&gt; t.Result);
    return totalBytes;
}
</code></pre><p>This orchestrator function essentially does the following:</p>
<ol>
<li>Takes a <code>rootDirectory</code> value as an input parameter.</li>
<li>Calls a function to get a recursive list of files under <code>rootDirectory</code>.</li>
<li>Makes multiple parallel function calls to upload each file into Azure Blob Storage.</li>
<li>Waits for all uploads to complete.</li>
<li>Returns the sum total bytes that were uploaded to Azure Blob Storage.</li>
</ol>
<p>The most interesting part of this function is the <code>await Task.WhenAll(tasks);</code> line. You may have noticed that all the calls to the <strong>E2_CopyFileToBlob</strong> function were <em>not</em> awaited. This is intentional to allow them to run in parallel. When we pass this array of tasks to <code>Task.WhenAll</code>, we get back a task which won&#39;t complete <em>until all the copy operations have completed</em>. If you&#39;re familiar with the Task Parallel Library (TPL) in .NET, then this is not new to you. The difference is that these tasks could be running on multiple VMs concurrently, and the extension ensures that the end-to-end execution is resilient to process recycling.</p>
<p>After awaiting from <code>Task.WhenAll</code>, we know that all function calls have completed and have returned values back to us. In this case, each call to <strong>E2_CopyFileToBlob</strong> will return the number of bytes uploaded, so calculating the sum total byte count is a matter of adding all those return values together.</p>
<h2 id="helper-activity-functions">Helper activity functions</h2>
<p>The helper activity functions, just as with other samples, are just regular functions that use the <code>activityTrigger</code> trigger binding. For example, the <strong>function.json</strong> file for <strong>E2_GetFileList</strong> looks like the following:</p>
<pre><code class="lang-json" name="Main">{
  &quot;bindings&quot;: [
    {
      &quot;name&quot;: &quot;rootDirectory&quot;,
      &quot;type&quot;: &quot;activityTrigger&quot;,
      &quot;direction&quot;: &quot;in&quot;
    }
  ],
  &quot;disabled&quot;: false
}
</code></pre><p>And here is the implementation:</p>
<pre><code class="lang-csharp" name="Main">#r &quot;Microsoft.Azure.WebJobs.Extensions.DurableTask&quot;

public static string[] Run(string rootDirectory, TraceWriter log)
{
    string[] files = Directory.GetFiles(rootDirectory, &quot;*&quot;, SearchOption.AllDirectories);
    log.Info($&quot;Found {files.Length} file(s) under {rootDirectory}.&quot;);

    return files;
}
</code></pre><div class="NOTE"><h5>Note</h5><p>You might be wondering why we couldn&#39;t just put this code directly into the orchestrator function. We could, but this would break one of the fundamental rules of orchestrator functions, which is that they should never do I/O, including local file system access.</p>
</div>
<p>The function.json for <strong>E2_CopyFileToBlob</strong> is similarly very simple:</p>
<pre><code class="lang-json" name="Main">{
  &quot;bindings&quot;: [
    {
      &quot;name&quot;: &quot;filePath&quot;,
      &quot;type&quot;: &quot;activityTrigger&quot;,
      &quot;direction&quot;: &quot;in&quot;
    }
  ],
  &quot;disabled&quot;: false
}
</code></pre><p>The implementation is also pretty straight forward. It happens to use some advanced features of Azure Functions bindings (i.e. the use of the <code>Binder</code> parameter), but you don&#39;t need to worry about those details for the purpose of this walkthrough.</p>
<pre><code class="lang-csharp" name="Main">#r &quot;Microsoft.Azure.WebJobs.Extensions.DurableTask&quot;
#r &quot;Microsoft.WindowsAzure.Storage&quot;

using Microsoft.WindowsAzure.Storage.Blob;

public static async Task&lt;long&gt; Run(
    string filePath,
    Binder binder,
    TraceWriter log)
{
    long byteCount = new FileInfo(filePath).Length;

    // strip the drive letter prefix and convert to forward slashes
    string blobPath = filePath
        .Substring(Path.GetPathRoot(filePath).Length)
        .Replace(&#39;\\&#39;, &#39;/&#39;);
    string outputLocation = $&quot;backups/{blobPath}&quot;;

    log.Info($&quot;Copying &#39;{filePath}&#39; to &#39;{outputLocation}&#39;. Total bytes = {byteCount}.&quot;);

    // copy the file contents into a blob
    using (Stream source = File.Open(filePath, FileMode.Open, FileAccess.Read, FileShare.ReadWrite))
    using (Stream destination = await binder.BindAsync&lt;CloudBlobStream&gt;(
        new BlobAttribute(outputLocation)))
    {
        await source.CopyToAsync(destination);
    }
    
    return byteCount;
}
</code></pre><p>The implementation loads the file from disk and asynchronously streams the contents into a blob of the same name. The return value is the number of bytes copied to storage, which is then used by the orchestrator function to compute the aggregate sum.</p>
<div class="NOTE"><h5>Note</h5><p>This is a perfect example of moving I/O operations into an <code>activityTrigger</code> function. Not only can the work be distributed across many different VMs, but we also get the benefits of checkpointing the progress, meaning we&#39;re able to automatically keep track of which uploads have already completed if the host process gets terminated for any reason.</p>
</div>
<h2 id="running-the-sample">Running the sample</h2>
<p>Using the HTTP-triggered functions included in the sample, you can start the orchestration using the below HTTP POST request.</p>
<pre><code class="lang-plaintext">POST http://{host}/orchestrators/E2_BackupSiteContent HTTP/1.1
Content-Type: application/json
Content-Length: 20

&quot;D:\\home\\LogFiles&quot;
</code></pre><div class="NOTE"><h5>Note</h5><p>The <strong>HttpStart</strong> function which you are invoking only works with JSON-formatted content. For this reason, the <code>Content-Type: application/json</code> header is required and the directory path is encoded as a JSON string.</p>
</div>
<p>This will trigger the <strong>E2_BackupSiteContent</strong> orchestrator and pass the string <code>D:\home\LogFiles</code> as a parameter. The response will provide a link to get the status of this backup operation:</p>
<pre><code class="lang-plaintext">HTTP/1.1 202 Accepted
Content-Length: 719
Content-Type: application/json; charset=utf-8
Location: http://{host}/admin/extensions/DurableTaskExtension/instances/b4e9bdcc435d460f8dc008115ff0a8a9?taskHub=DurableFunctionsHub&amp;connection=Storage&amp;code={systemKey}

(...trimmed...)
</code></pre><p>Depending on how many log files you have in your function app, this operation could take several minutes to complete. You can get the latest status by querying the URL in the <code>Location</code> header of the previous HTTP 202 response.</p>
<pre><code class="lang-plaintext">GET http://{host}/admin/extensions/DurableTaskExtension/instances/b4e9bdcc435d460f8dc008115ff0a8a9?taskHub=DurableFunctionsHub&amp;connection=Storage&amp;code={systemKey}
</code></pre><pre><code class="lang-plaintext">HTTP/1.1 202 Accepted
Content-Length: 148
Content-Type: application/json; charset=utf-8
Location: http://{host}/admin/extensions/DurableTaskExtension/instances/b4e9bdcc435d460f8dc008115ff0a8a9?taskHub=DurableFunctionsHub&amp;connection=Storage&amp;code={systemKey}

{&quot;runtimeStatus&quot;:&quot;Running&quot;,&quot;input&quot;:&quot;D:\\home\\LogFiles&quot;,&quot;output&quot;:null,&quot;createdTime&quot;:&quot;2017-06-29T18:50:55Z&quot;,&quot;lastUpdatedTime&quot;:&quot;2017-06-29T18:51:16Z&quot;}
</code></pre><p>In this case, the function is still running. We are also able to see the input that was saved into the orchestrator state and the last updated time. We can continue to use the <code>Location</code> header values to poll for completion. Once complete, we can expect to see an HTTP response value similar to the following:</p>
<pre><code class="lang-plaintext">HTTP/1.1 200 OK
Content-Length: 152
Content-Type: application/json; charset=utf-8

{&quot;runtimeStatus&quot;:&quot;Completed&quot;,&quot;input&quot;:&quot;D:\\home\\LogFiles&quot;,&quot;output&quot;:452071,&quot;createdTime&quot;:&quot;2017-06-29T18:50:55Z&quot;,&quot;lastUpdatedTime&quot;:&quot;2017-06-29T18:51:26Z&quot;}
</code></pre><p>Now we can see that the orchestration is complete and approximately how much time it took to complete. We now also see a value for the <code>output</code> field, which indicates that around 450 KB of logs were uploaded.</p>
<h2 id="wrapping-up">Wrapping up</h2>
<p>At this point, you should have a greater understanding of the core orchestration capabilities of Durable Functions. Subsequent samples will go into more advanced features and scenarios.</p>
<h2 id="full-sample-code">Full Sample Code</h2>
<p>Here is the full orchestration as a single C# file using the Visual Studio project syntax:</p>
<pre><code class="lang-csharp" name="Main">// Copyright (c) .NET Foundation. All rights reserved.
// Licensed under the MIT License. See License.txt in the project root for license information.

using System.IO;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.Azure.WebJobs;
using Microsoft.Azure.WebJobs.Host;
using Microsoft.WindowsAzure.Storage.Blob;

namespace VSSample
{
    public static class BackupSiteContent
    {
        [FunctionName(&quot;E2_BackupSiteContent&quot;)]
        public static async Task&lt;long&gt; Run(
            [OrchestrationTrigger] DurableOrchestrationContext backupContext)
        {
            string rootDirectory = backupContext.GetInput&lt;string&gt;()?.Trim();
            if (string.IsNullOrEmpty(rootDirectory))
            {
                rootDirectory = Directory.GetParent(typeof(BackupSiteContent).Assembly.Location).FullName;
            }

            string[] files = await backupContext.CallActivityAsync&lt;string[]&gt;(
                &quot;E2_GetFileList&quot;,
                rootDirectory);

            var tasks = new Task&lt;long&gt;[files.Length];
            for (int i = 0; i &lt; files.Length; i++)
            {
                tasks[i] = backupContext.CallActivityAsync&lt;long&gt;(
                    &quot;E2_CopyFileToBlob&quot;,
                    files[i]);
            }

            await Task.WhenAll(tasks);

            long totalBytes = tasks.Sum(t =&gt; t.Result);
            return totalBytes;
        }

        [FunctionName(&quot;E2_GetFileList&quot;)]
        public static string[] GetFileList(
            [ActivityTrigger] string rootDirectory, 
            TraceWriter log)
        {
            log.Info($&quot;Searching for files under &#39;{rootDirectory}&#39;...&quot;);
            string[] files = Directory.GetFiles(rootDirectory, &quot;*&quot;, SearchOption.AllDirectories);
            log.Info($&quot;Found {files.Length} file(s) under {rootDirectory}.&quot;);

            return files;
        }

        [FunctionName(&quot;E2_CopyFileToBlob&quot;)]
        public static async Task&lt;long&gt; CopyFileToBlob(
            [ActivityTrigger] string filePath,
            Binder binder,
            TraceWriter log)
        {
            long byteCount = new FileInfo(filePath).Length;

            // strip the drive letter prefix and convert to forward slashes
            string blobPath = filePath
                .Substring(Path.GetPathRoot(filePath).Length)
                .Replace(&#39;\\&#39;, &#39;/&#39;);
            string outputLocation = $&quot;backups/{blobPath}&quot;;

            log.Info($&quot;Copying &#39;{filePath}&#39; to &#39;{outputLocation}&#39;. Total bytes = {byteCount}.&quot;);

            // copy the file contents into a blob
            using (Stream source = File.Open(filePath, FileMode.Open, FileAccess.Read, FileShare.Read))
            using (Stream destination = await binder.BindAsync&lt;CloudBlobStream&gt;(
                new BlobAttribute(outputLocation, FileAccess.Write)))
            {
                await source.CopyToAsync(destination);
            }

            return byteCount;
        }
    }
}
</code></pre></article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                  <li>
                    <a href="https://github.com/Azure/azure-functions-durable-extension/blob/master/docfx/articles/samples/cloud-backup.md/#L1" class="contribution-link">Improve this Doc</a>
                  </li>
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
              <!-- <p><a class="back-to-top" href="#top">Back to top</a><p> -->
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            Copyright © .NET Foundation
            
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../../styles/docfx.js"></script>
    <script type="text/javascript" src="../../styles/main.js"></script>
  </body>
</html>
